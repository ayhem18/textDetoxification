{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import os, sys\n",
    "from pathlib import Path\n",
    "HOME = os.getcwd()\n",
    "DATA_FOLDER = os.path.join(Path(HOME).parent, 'data')\n",
    "data_path = os.path.join(DATA_FOLDER, 'fixed.csv')\n",
    "\n",
    "current = HOME \n",
    "while 'src' not in os.listdir(current):\n",
    "    current = Path(current).parent\n",
    "\n",
    "sys.path.append(str(current))\n",
    "sys.path.append(os.path.join(str(current), 'data_analysis'))\n",
    "sys.path.append(os.path.join(str(current), 'evaluation'))\n",
    "sys.path.append(os.path.join(str(current), 'text_processing'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 597521/597521 [00:01<00:00, 536047.55 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# import the all_data_processed.csv file.\n",
    "from datasets import load_dataset\n",
    "all_data = load_dataset('csv', data_files=os.path.join(DATA_FOLDER, 'all_data_processed.csv'), split='train')\n",
    "# make sure to filter any None values\n",
    "all_data = all_data.filter(lambda s: (isinstance(s['source'], str) and isinstance(s['target'], str)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step2: create the model and the tokenizer\n",
    "import torch\n",
    "from transformers import T5TokenizerFast, AutoModelForSeq2SeqLM\n",
    "CHECKPOINT = 't5-small'\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "tokenizer = T5TokenizerFast.from_pretrained(CHECKPOINT)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(CHECKPOINT).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 597519/597519 [01:03<00:00, 9426.15 examples/s] \n",
      "Creating CSV from Arrow format: 100%|██████████| 277/277 [00:01<00:00, 245.36ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "31406141"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.text_processing import preprocess as pr\n",
    "# the next step is to filter the dataset\n",
    "def filter_data(sample):\n",
    "    \"\"\"This function receives  a batch of samples from the original data and filters those whose 'source' text is shorter than the 'target' text.\"\"\"\n",
    "    # first tokenize each 'source' and 'target' fields\n",
    "    source = pr.tokenize(sample['source'], tokenizer_type='word')\n",
    "    target = pr.tokenize(sample['target'], tokenizer_type='word')\n",
    "    return len(source) > len(target)\n",
    "\n",
    "summary_data = all_data.filter(filter_data)\n",
    "# save the data\n",
    "summary_data.to_csv(os.path.join(DATA_FOLDER, 'summarized_data.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 276516/276516 [00:07<00:00, 39445.29 examples/s]\n",
      "Map: 100%|██████████| 276516/276516 [00:06<00:00, 43389.16 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# step3 create a function to tokenize the data\n",
    "TASK_PREFIX = 'summarize: '\n",
    "def prepare_data(batch, split: str ='source'):\n",
    "    tok_batch = [TASK_PREFIX + s for s in batch[split]]\n",
    "    return tokenizer(tok_batch, truncation=True)\n",
    "\n",
    "source_tokenized = summary_data.map(prepare_data, batched=True)\n",
    "target_tokenized = summary_data.map(lambda x: prepare_data(x, split='target'), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['source', 'target', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 276516\n",
      "})\n",
      "Dataset({\n",
      "    features: ['source', 'target', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 276516\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(source_tokenized)\n",
    "# 'soure' and 'target' columns are unnecessary for the 'source_tokenized' dataset\n",
    "print(target_tokenized)\n",
    "# 'source', 'target, and 'attention_masks' are unncessary for the 'target_tokenized'\n",
    "source_tokenized = source_tokenized.remove_columns(['source', 'target'])\n",
    "target_tokenized = target_tokenized.remove_columns(['source', 'target', 'attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create 2 dataloaders, one for source (will be used for predictions) and one for target\n",
    "import torch\n",
    "from transformers import DataCollatorWithPadding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "# create a dataloader to pass the data to the model\n",
    "source_dl = DataLoader(dataset=source_tokenized, batch_size=64, shuffle=False, collate_fn=data_collator)\n",
    "target_dl = DataLoader(dataset=target_tokenized, batch_size=64, shuffle=False, collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluation import toxicity_classication as tc\n",
    "import importlib\n",
    "importlib.reload(tc)\n",
    "import re\n",
    "\n",
    "def build_dataset():\n",
    "    for source_b, target_b in zip(source_dl, target_dl):\n",
    "        # ignore the source data\n",
    "        model_batch = {k: v.to(DEVICE) for k, v in source_b.items()}\n",
    "        # pass the batch to the model\n",
    "        output = model.generate(**model_batch)\n",
    "        # print(output)\n",
    "        output_decoded = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
    "        source = tokenizer.batch_decode(source_b['input_ids'], skip_special_tokens=True)\n",
    "        target = tokenizer.batch_decode(target_b['input_ids'], skip_special_tokens=True)\n",
    "\n",
    "        # the summary's toxicity classification is next:\n",
    "        summary_tox = tc.toxic_classification(output_decoded)\n",
    "        source_tox = tc.toxic_classification(source)\n",
    "        for text, source_text, target_text, tox, s_tox in zip(output_decoded, source, target, summary_tox, source_tox):\n",
    "            yield {\"source\": re.sub(TASK_PREFIX, \"\", source_text), \"target\": re.sub(TASK_PREFIX, \"\", target_text), \"summary\": text, \"summary_tox\": tox, \"source_tox\":s_tox}\n",
    "\n",
    "    \n",
    "summarized_dataset = Dataset.from_generator(build_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarized_dataset.to_csv(os.path.join(DATA_FOLDER, 'summarized.csv'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
