{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ayhem18/DEV/TextDetoxification/env/lib/python3.11/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os, sys\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from pathlib import Path\n",
    "\n",
    "HOME = os.getcwd()\n",
    "current = HOME \n",
    "while 'src' not in os.listdir(current):\n",
    "    current = Path(current).parent\n",
    "\n",
    "PARENT_DIR = str(current)\n",
    "sys.path.append(str(current))\n",
    "sys.path.append(os.path.join(str(current), 'data_analysis'))\n",
    "sys.path.append(os.path.join(str(current), 'evaluation'))\n",
    "sys.path.append(os.path.join(str(current), 'text_processing'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = os.path.join(PARENT_DIR, 'src', 'data')\n",
    "NOTEBOOK_DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "CHECKPOINT = 'distilbert-base-uncased'  # let's keep it simple as for the first iteration\n",
    "TOKENIZER = AutoTokenizer.from_pretrained(CHECKPOINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|██████████| 1/1 [00:00<00:00, 17549.39it/s]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 1808.67it/s]\n",
      "Generating train split: 597519 examples [00:00, 708018.22 examples/s]\n",
      "Filter: 100%|██████████| 597519/597519 [00:01<00:00, 536018.70 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 598/598 [00:02<00:00, 244.45ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "66239054"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's focus\n",
    "from datasets import load_dataset\n",
    "data = load_dataset('csv', data_files=os.path.join(DATA_FOLDER, 'all_data_processed.csv'), split='train')\n",
    "data = data.filter(function=lambda b: (isinstance(b['source'], str) and isinstance(b['target'], str)))\n",
    "data.to_csv(os.path.join(DATA_FOLDER, 'all_data_processed.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data \\\n",
    "import src.data_preparation.prepare_data as pdr\n",
    "train_data, val_data, test_data = pdr.data_split(all_data=data.select(range(2000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['source', 'target'],\n",
       "     num_rows: 1920\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['source', 'target'],\n",
       "     num_rows: 40\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['source', 'target'],\n",
       "     num_rows: 39\n",
       " }))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data, val_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/1920 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1920/1920 [00:00<00:00, 10745.51 examples/s]\n",
      "Map: 100%|██████████| 40/40 [00:00<00:00, 7319.90 examples/s]\n",
      "Map: 100%|██████████| 39/39 [00:00<00:00, 7803.54 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def prepare_labeled_data(batch):\n",
    "    model_inputs = TOKENIZER(batch['source'], truncation=True, max_length=1024)\n",
    "    labels = TOKENIZER(batch['target'],truncation=True, max_length=1024)\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    # data['labels_attention_mask'] = labels['attention_mask']\n",
    "    return model_inputs\n",
    "\n",
    "train_data = train_data.map(prepare_labeled_data, batched=True).remove_columns(['source', 'target'])\n",
    "val_data = val_data.map(prepare_labeled_data, batched=True).remove_columns(['source', 'target'])\n",
    "test_data = test_data.map(prepare_labeled_data, batched=True).remove_columns(['source', 'target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 2396, 1010, 2115, 2269, 1010, 102],\n",
       "  [101, 26450, 2068, 2035, 999, 102]],\n",
       " 'attention_mask': [[1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]],\n",
       " 'labels': [[101, 5506, 2332, 1010, 2115, 2269, 1012, 102],\n",
       "  [101, 5495, 2068, 2035, 2039, 999, 102]]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_padding_data_collator(batch):\n",
    "    # let's pad and see how it goes\n",
    "    max_len = len(max([i['input_ids'] for i in batch], key=len))\n",
    "    max_target_len = len(max([i['labels'] for i in batch], key=len))\n",
    "\n",
    "\n",
    "    for e in batch:\n",
    "        e['input_ids'] = e['input_ids'] + [TOKENIZER.pad_token_id] * (max_len - len(e['input_ids']))\n",
    "        e['attention_mask'] = e['attention_mask'] + [0] * (max_len - len(e['input_ids']))\n",
    "        e['labels'] = e['labels'] + [TOKENIZER.pad_token_id] * (max_target_len - len(e ['labels']))    \n",
    "    # padd the target\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apparently, I have to write my\n",
    "# def custom_padding_data_collator(batch):\n",
    "#     # find the longest sentence in the batch\n",
    "#     max_len = len(max(batch['input_ids'], key=len))\n",
    "#     # make sure to add padding to all sentences\n",
    "#     batch['input_ids'] = [(ids + TOKENIZER.pad_token_id * max_len - len(ids)) for ids in batch['input_ids']]\n",
    "    \n",
    "#     # add '0's to the attention masks\n",
    "#     batch['attention_mask'] = [(mask + 0 * max_len - len(mask)) for mask in batch['attention_mask']]\n",
    "\n",
    "#     # pad the labels\n",
    "#     max_target = len(max(batch['labels'], key=len)) \n",
    "#     batch['labels'] = [(ids + TOKENIZER.pad_token_id * max_target - len(ids)) for ids in batch['labels']]\n",
    "\n",
    "#     return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are now ready to create the dataloader\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "# dc = DataCollatorWithPadding(tokenizer=TOKENIZER)\n",
    "dc = DataCollatorForSeq2Seq(tokenizer=TOKENIZER, model=AutoModel.from_pretrained(CHECKPOINT))\n",
    "train_dl = DataLoader(dataset=train_data, batch_size=4, shuffle=True, collate_fn=custom_padding_data_collator)\n",
    "val_dl = DataLoader(dataset=val_data, batch_size=4, shuffle=False, collate_fn=custom_padding_data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'input_ids': [101,\n",
       "   2115,\n",
       "   3611,\n",
       "   1005,\n",
       "   1055,\n",
       "   1037,\n",
       "   19101,\n",
       "   1010,\n",
       "   4845,\n",
       "   1012,\n",
       "   102,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "  'labels': [101,\n",
       "   2115,\n",
       "   6643,\n",
       "   1010,\n",
       "   2002,\n",
       "   1005,\n",
       "   1055,\n",
       "   2288,\n",
       "   1037,\n",
       "   2502,\n",
       "   2677,\n",
       "   1012,\n",
       "   2879,\n",
       "   1012,\n",
       "   102]},\n",
       " {'input_ids': [101,\n",
       "   2045,\n",
       "   1005,\n",
       "   1055,\n",
       "   1037,\n",
       "   2843,\n",
       "   1997,\n",
       "   16034,\n",
       "   1999,\n",
       "   2023,\n",
       "   2388,\n",
       "   11263,\n",
       "   9102,\n",
       "   1012,\n",
       "   102,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "  'labels': [101,\n",
       "   2045,\n",
       "   1005,\n",
       "   1055,\n",
       "   1037,\n",
       "   2843,\n",
       "   1997,\n",
       "   16034,\n",
       "   1999,\n",
       "   2032,\n",
       "   1012,\n",
       "   102,\n",
       "   0,\n",
       "   0,\n",
       "   0]},\n",
       " {'input_ids': [101,\n",
       "   1045,\n",
       "   2228,\n",
       "   1037,\n",
       "   10515,\n",
       "   2003,\n",
       "   1037,\n",
       "   9951,\n",
       "   5949,\n",
       "   1997,\n",
       "   2051,\n",
       "   1012,\n",
       "   102,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "  'labels': [101,\n",
       "   1045,\n",
       "   2228,\n",
       "   8720,\n",
       "   2003,\n",
       "   1037,\n",
       "   5949,\n",
       "   1997,\n",
       "   2051,\n",
       "   1012,\n",
       "   102,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0]},\n",
       " {'input_ids': [101,\n",
       "   2065,\n",
       "   2009,\n",
       "   2003,\n",
       "   1996,\n",
       "   2259,\n",
       "   4665,\n",
       "   1010,\n",
       "   2057,\n",
       "   2442,\n",
       "   6033,\n",
       "   2014,\n",
       "   2012,\n",
       "   2320,\n",
       "   1010,\n",
       "   2909,\n",
       "   1012,\n",
       "   102],\n",
       "  'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "  'labels': [101,\n",
       "   2065,\n",
       "   2008,\n",
       "   2003,\n",
       "   1996,\n",
       "   2553,\n",
       "   1010,\n",
       "   2057,\n",
       "   2442,\n",
       "   6033,\n",
       "   2009,\n",
       "   3202,\n",
       "   1012,\n",
       "   102,\n",
       "   0]}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_dl))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30522"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "import src.models.customSeq2Seq.seq2seq_model as s2s\n",
    "import src.models.customSeq2Seq.train as ts2s\n",
    "import importlib\n",
    "importlib.reload(s2s)\n",
    "importlib.reload(ts2s)\n",
    "# the output size should be that of the vocabulary size\n",
    "TOKENIZER.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "DecoderRNN.__init__() missing 2 required positional arguments: 'emb_dim' and 'output_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/ayhem18/DEV/TextDetoxification/src/models/customSeq2Seq/train_s2s.ipynb Cell 13\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ayhem18/DEV/TextDetoxification/src/models/customSeq2Seq/train_s2s.ipynb#X14sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m encoder \u001b[39m=\u001b[39m s2s\u001b[39m.\u001b[39mBertBasedEncoder(hidden_dim\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m, num_layers\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ayhem18/DEV/TextDetoxification/src/models/customSeq2Seq/train_s2s.ipynb#X14sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# the output of the encoder\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/ayhem18/DEV/TextDetoxification/src/models/customSeq2Seq/train_s2s.ipynb#X14sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m decoder \u001b[39m=\u001b[39m s2s\u001b[39m.\u001b[39;49mDecoderRNN(token_classifier\u001b[39m=\u001b[39;49mclassifier)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ayhem18/DEV/TextDetoxification/src/models/customSeq2Seq/train_s2s.ipynb#X14sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m e_opt \u001b[39m=\u001b[39m Adam(encoder\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m0.01\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ayhem18/DEV/TextDetoxification/src/models/customSeq2Seq/train_s2s.ipynb#X14sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m d_opt \u001b[39m=\u001b[39m Adam(decoder\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m0.01\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: DecoderRNN.__init__() missing 2 required positional arguments: 'emb_dim' and 'output_size'"
     ]
    }
   ],
   "source": [
    "# let's build the classifier for the Token prediction\n",
    "from torch import nn\n",
    "from src.models.customSeq2Seq.classification_head import ExponentialClassifier\n",
    "classifier = ExponentialClassifier(num_classes=TOKENIZER.vocab_size, in_features=100, num_layers=5)\n",
    "encoder = s2s.BertBasedEncoder(hidden_dim=100, num_layers=2)\n",
    "# the output of the encoder\n",
    "decoder = s2s.DecoderRNN(token_classifier=classifier)\n",
    "\n",
    "\n",
    "e_opt = Adam(encoder.parameters(), lr=0.01)\n",
    "d_opt = Adam(decoder.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "from src.models.customSeq2Seq.train import train_model\n",
    "train_model(encoder=encoder, decoder=decoder, train_dataloader=train_dl, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
