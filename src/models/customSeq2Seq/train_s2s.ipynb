{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os, sys\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from pathlib import Path\n",
    "\n",
    "HOME = os.getcwd()\n",
    "current = HOME \n",
    "while 'src' not in os.listdir(current):\n",
    "    current = Path(current).parent\n",
    "\n",
    "PARENT_DIR = str(current)\n",
    "sys.path.append(str(current))\n",
    "sys.path.append(os.path.join(str(current), 'data_analysis'))\n",
    "sys.path.append(os.path.join(str(current), 'evaluation'))\n",
    "sys.path.append(os.path.join(str(current), 'text_processing'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "DATA_FOLDER = os.path.join(PARENT_DIR, 'src', 'data')\n",
    "NOTEBOOK_DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "CHECKPOINT = 'roberta-base'  # let's keep it simple as for the first iteration\n",
    "TOKENIZER = AutoTokenizer.from_pretrained(CHECKPOINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# let's focus\n",
    "from datasets import load_dataset\n",
    "data = load_dataset('csv', data_files=os.path.join(DATA_FOLDER, 'all_data_processed.csv'), split='train')\n",
    "data = data.filter(function=lambda b: (isinstance(b['source'], str) and isinstance(b['target'], str)))\n",
    "data.to_csv(os.path.join(DATA_FOLDER, 'all_data_processed.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# split the data \\\n",
    "import src.data_preparation.prepare_data as pdr\n",
    "train_data, val_data, test_data = pdr.data_split(all_data=data.select(range(2000)))\n",
    "train_data, val_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "def prepare_labeled_data(batch):\n",
    "    model_inputs = TOKENIZER(batch['source'], truncation=True, max_length=1024)\n",
    "    labels = TOKENIZER(batch['target'],truncation=True, max_length=1024)\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    # data['labels_attention_mask'] = labels['attention_mask']\n",
    "    return model_inputs\n",
    "\n",
    "train_data = train_data.map(prepare_labeled_data, batched=True).remove_columns(['source', 'target'])\n",
    "val_data = val_data.map(prepare_labeled_data, batched=True).remove_columns(['source', 'target'])\n",
    "test_data = test_data.map(prepare_labeled_data, batched=True).remove_columns(['source', 'target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# we are now ready to create the dataloader\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorForSeq2Seq, DataCollatorWithPadding\n",
    "# dc = DataCollatorWithPadding(tokenizer=TOKENIZER)\n",
    "dc = DataCollatorForSeq2Seq(tokenizer=TOKENIZER, model=AutoModel.from_pretrained(CHECKPOINT))\n",
    "train_dl = DataLoader(dataset=train_data, batch_size=4, shuffle=True, collate_fn=dc)\n",
    "val_dl = DataLoader(dataset=val_data, batch_size=4, shuffle=False, collate_fn=dc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "next(iter(train_dl))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# let's build the classifier for the Token prediction\n",
    "import importlib\n",
    "from torch import nn\n",
    "import src.models.customSeq2Seq.model as s2s \n",
    "from src.models.customSeq2Seq.classification_head import ExponentialClassifier\n",
    "from torch.optim import AdamW\n",
    "importlib.reload(s2s)\n",
    "\n",
    "classifier = ExponentialClassifier(num_classes=TOKENIZER.vocab_size, in_features=768, num_layers=5)\n",
    "encoder = s2s.RobertaBasedEncoder()\n",
    "# the output of the encoder\n",
    "decoder = s2s.RobertaBasedDecoder(token_classifier=classifier, num_layers=2)\n",
    "e_opt = AdamW(encoder.parameters(), lr=0.01)\n",
    "d_opt = AdamW(decoder.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "from seq2seq_model import BertBasedEncoder, DecoderRNN\n",
    "from typing import Optional, Union\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from src.training_utilities.exp_tracking import create_summary_writer, _add_metric, report_results\n",
    "from src.training_utilities.pytorch_utilities import save_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "import train as tr\n",
    "importlib.reload(tr)\n",
    "\n",
    "def train_model(\n",
    "                encoder: BertBasedEncoder, \n",
    "                decoder: DecoderRNN,\n",
    "                train_dataloader: DataLoader[torch.Tensor],\n",
    "                test_dataloader: DataLoader[torch.Tensor],\n",
    "                loss_function,\n",
    "                e_opt, \n",
    "                d_opt,\n",
    "                epochs: int = 5,\n",
    "                log_dir: Optional[Union[Path, str]] = None,\n",
    "                save_path: Optional[Union[Path, str]] = None,\n",
    "                ):\n",
    "\n",
    "    save_path = save_path if save_path is not None else log_dir    \n",
    "\n",
    "    # best_model, best_loss = None, None\n",
    "    min_training_loss, no_improve_counter, best_model = float('inf'), 0, None\n",
    "\n",
    "    # before proceeding with the training, let's set the summary writer\n",
    "    writer, log_dir = (None, None) if (log_dir is None) else create_summary_writer(log_dir, return_path=True)\n",
    "\n",
    "    for _ in tqdm(range(epochs)):\n",
    "        epoch_train_loss, epoch_train_acc = tr.train_per_epoch(encoder=encoder,\n",
    "                                                            decoder=decoder,\n",
    "                                                            e_opt=e_opt,\n",
    "                                                            d_opt=d_opt,\n",
    "                                                            train_dataloader=train_dataloader,\n",
    "                                                            loss_function=loss_function)\n",
    "\n",
    "        epoch_val_loss, epoch_val_acc = tr.val_per_epoch(encoder=encoder,\n",
    "                                                      decoder=decoder,\n",
    "                                                      dataloader=test_dataloader,\n",
    "                                                      loss_function=loss_function)\n",
    "\n",
    "        no_improve_counter = no_improve_counter + 1 if min_training_loss < epoch_train_loss else 0\n",
    "\n",
    "        if min_training_loss > epoch_train_loss:\n",
    "            # save the model with the lowest training error\n",
    "            min_training_loss = epoch_train_loss\n",
    "\n",
    "        report_results(train_losses_dict={\"train_loss\": epoch_train_loss, \"train_acc\": epoch_train_acc}, \n",
    "                        val_losses_dict={\"val_accuracy\": epoch_val_acc, \"val_loss\": epoch_val_loss})\n",
    "\n",
    "        _add_metric(writer,\n",
    "                    tag='loss', \n",
    "                    values={\"train_loss\": epoch_train_loss, \"val_loss\": epoch_val_loss},\n",
    "                    epoch=_)\n",
    "\n",
    "        _add_metric(writer,\n",
    "                    tag='accuracy', \n",
    "                    values={\"train_loss\": epoch_train_acc, \"val_loss\": epoch_val_acc},\n",
    "                    epoch=_)\n",
    "        \n",
    "        if epoch_train_loss < min_training_loss: \n",
    "            best_model = (encoder, decoder)\n",
    "\n",
    "    # save the best combination at the end\n",
    "    save_model(model=encoder, path=os.path.join(save_path, 'encoder.pt'))\n",
    "    save_model(model=decoder, path=os.path.join(save_path, 'decoder.pt'))\n",
    "\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import src.models.customSeq2Seq.train as tr\n",
    "import src.models.customSeq2Seq.model as s2s\n",
    "\n",
    "importlib.reload(tr)\n",
    "importlib.reload(s2s)\n",
    "\n",
    "train_model(encoder=encoder, \n",
    "            decoder=decoder, \n",
    "            train_dataloader=train_dl,\n",
    "            test_dataloader=val_dl, \n",
    "            e_opt=e_opt, \n",
    "            d_opt=d_opt, \n",
    "            loss_function=criterion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
