{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the notebook's main objective is to filter and prepare the dataset to train a summarizer on it.\n",
    "\n",
    "import os, sys\n",
    "from pathlib import Path\n",
    "HOME = os.getcwd()\n",
    "DATA_FOLDER = os.path.join(Path(HOME).parent, 'data')\n",
    "data_path = os.path.join(DATA_FOLDER, 'filtered.tsv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "current = HOME \n",
    "while 'src' not in os.listdir(current):\n",
    "    current = Path(current).parent\n",
    "\n",
    "sys.path.append(str(current))\n",
    "sys.path.append(os.path.join(str(current), 'data_analysis'))\n",
    "sys.path.append(os.path.join(str(current), 'evaluation'))\n",
    "sys.path.append(os.path.join(str(current), 'text_processing'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ayhem18/DEV/TextDetoxification/env/lib/python3.11/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# let's load the data as a HF Dataset\n",
    "from datasets import load_dataset\n",
    "original_data = load_dataset('csv', data_files=os.path.join(DATA_FOLDER, 'fixed.csv'), split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['source', 'target'],\n",
       "    num_rows: 577777\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's start with removing all fields but 'source' and 'target'ArithmeticError\n",
    "original_data = original_data.remove_columns(['source_tox', 'target_tox', 'similarity', 'lenght_diff'])\n",
    "original_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Original Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['source', 'target'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = original_data.shuffle(seed=69).select(range(1000))\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.text_processing import preprocess as pr\n",
    "def process_text(text: str) -> str:\n",
    "    return pr.no_extra_spaces(pr.no_extra_chars(pr.to_lower(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the first step is to process the data \n",
    "def process_sample_text(batch):\n",
    "    \"\"\"This function recieves  batch of samples from the original data. It returns a new batch where each\n",
    "    'source' and 'target' text data will be processed using the function above\n",
    "    \"\"\"\n",
    "    new_batch = dict([(k, [process_text(t) for t in v]) for k, v in batch.items()])\n",
    "    return new_batch\n",
    "\n",
    "sample = sample.map(process_sample_text, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the next step is to filter the dataset\n",
    "def filter_data(sample):\n",
    "    \"\"\"This function receives  a batch of samples from the original data and filters those whose 'source' text is shorter than the 'target' text.\"\"\"\n",
    "    # first tokenize each 'source' and 'target' fields\n",
    "    source = pr.tokenize(sample['source'], tokenizer_type='word')\n",
    "    target = pr.tokenize(sample['target'], tokenizer_type='word')\n",
    "    return len(source) > len(target)\n",
    "\n",
    "sample = sample.filter(filter_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['source', 'target'],\n",
       "    num_rows: 458\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auxiliary Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import set_seed\n",
    "# set the seed for reproducibility, set_seed(69)\n",
    "aux_data = load_dataset('s-nlp/paradetox')['train'] # the dataset has only one split: 'train'\n",
    "# shuffle the data\n",
    "aux_data = aux_data.rename_column('en_toxic_comment', 'source').rename_column('en_neutral_comment', 'target').shuffle(seed=69)\n",
    "# split the data\n",
    "train_split, val_split, test_split = aux_data.select(range(2000, len(aux_data))), aux_data.select(range(1000)), aux_data.select(range(1000, 2000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['source', 'target'],\n",
       "     num_rows: 17744\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['source', 'target'],\n",
       "     num_rows: 1000\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['source', 'target'],\n",
       "     num_rows: 1000\n",
       " }))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_split, val_split, test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets\n",
    "train_data, val_data, test_data = concatenate_datasets([train_split, sample]), val_split, test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the next step is to tokenize the data  \n",
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "CHECKPOINT = 't5-small'\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "TOKENIZER = AutoTokenizer.from_pretrained(CHECKPOINT)\n",
    "MODEL = AutoModelForSeq2SeqLM.from_pretrained(CHECKPOINT).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK_PREFIX = 'summarize: '\n",
    "\n",
    "def prepare_labeled_data(batch):\n",
    "    # add the task predix to each sentence\n",
    "    inputs = [TASK_PREFIX + doc for doc in batch[\"source\"]]\n",
    "    # tokenize 'x'\n",
    "    model_inputs = TOKENIZER(inputs, truncation=True, max_length=1028)\n",
    "    # tokenize 'y'  \n",
    "    labels = TOKENIZER(text_target=batch[\"target\"], truncation=True, max_length=124)\n",
    "    # add it to the model's input\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.map(prepare_labeled_data, batched=True).remove_columns(['source', 'target'])\n",
    "val_data = val_data.map(prepare_labeled_data, batched=True).remove_columns(['source', 'target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data\n",
    "# let's choose a small portion of the data to experiment with\n",
    "train_data = train_data.shuffle(seed=69).select(range(2000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a DataCollator for padding for the sequence to sequence models\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=TOKENIZER, model=CHECKPOINT)\n",
    "# we are now ready to create the dataloader\n",
    "from torch.utils.data import DataLoader\n",
    "train_dl = DataLoader(dataset=train_data, batch_size=4, shuffle=True, collate_fn=data_collator)\n",
    "val_dl = DataLoader(dataset=val_data, batch_size=4, shuffle=False, collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "# make sure the data is loaded correctly\n",
    "b1, b2 = next(iter(train_dl)), next(iter(val_dl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# let's build the training loop: first the train_per_epoch function\n",
    "def train_per_epoch(train_dataloader, \n",
    "                    summary_model: AutoModelForSeq2SeqLM, # the summarization model to customize \n",
    "                    summary_tokenizer: AutoTokenizer, \n",
    "                    toxicity_loss_function: callable, \n",
    "                    toxicity_coeff: float,\n",
    "                    optimizer: torch.optim, \n",
    "                    scheduler: torch.optim.lr_scheduler,\n",
    "                    device: str,\n",
    "                    ):\n",
    "    \n",
    "    # make sure to save the reesults\n",
    "    summary_loss = 0\n",
    "    toxic_loss = 0\n",
    "    train_loss = 0\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "        # first put all the data into the device\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        # the next step is to pass the data to the model \n",
    "        model_output = summary_model(**batch)\n",
    "        # extract the sequence to sequence loss\n",
    "        sq2sq_loss = model_output.loss \n",
    "        \n",
    "        # save the sequence to sequence loss\n",
    "        summary_loss += sq2sq_loss.item()\n",
    "\n",
    "        # time to build the toxicity loss\n",
    "        # 1. generate the output in textual form: this can be done through the generate model, so it expects different input\n",
    "        batch_generate = {k: v for k, v in batch.items() if k != 'labels'}    \n",
    "        output_decoded = summary_tokenizer.batch_decode(summary_model.generate(**batch_generate), skip_special_tokens=True)\n",
    "        tc_loss = toxicity_loss_function(output_decoded, device=device)\n",
    "        \n",
    "        # save the toxicity loss\n",
    "        toxic_loss += tc_loss.item()\n",
    "\n",
    "        # the final loss is a linear a combination of the seq2seq loss and the classification loss\n",
    "        final_loss = sq2sq_loss + tc_loss * toxicity_coeff\n",
    "        train_loss += final_loss\n",
    "\n",
    "        # the usual part of the training loop\n",
    "        final_loss.backward() \n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    # make sure to average the loss across the different batches\n",
    "    summary_loss /= len(train_dataloader)\n",
    "    toxic_loss /= len(train_dataloader)\n",
    "    train_loss /= len(train_dataloader)\n",
    "    \n",
    "    \n",
    "    # make sure to return the losses\n",
    "    return {\"summary_loss\": summary_loss, \"toxic_loss\": toxic_loss, \"train_loss\": train_loss}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's define the validation epoch\n",
    "def val_per_epoch(val_dataloader, \n",
    "                    summary_model: AutoModelForSeq2SeqLM, # the summarization model to customize \n",
    "                    summary_tokenizer: AutoTokenizer, \n",
    "                    toxicity_loss_function: callable, \n",
    "                    toxicity_coeff: float,\n",
    "                    device: str\n",
    "                    ):    \n",
    "    summary_loss = 0\n",
    "    toxic_loss = 0\n",
    "    val_loss = 0\n",
    "\n",
    "    for batch in val_dataloader:\n",
    "        # first put all the data into the device\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        # the next step is to pass the data to the model \n",
    "        model_output = summary_model(**batch)\n",
    "        # extract the sequence to sequence loss\n",
    "        sq2sq_loss = model_output.loss \n",
    "        \n",
    "        # save the sequence to sequence loss\n",
    "        summary_loss += sq2sq_loss.item()\n",
    "\n",
    "        # time to build the toxicity loss\n",
    "        # 1. generate the output in textual form: this can be done through the generate model, so it expects different input\n",
    "        batch_generate = {k: v for k, v in batch.items() if k != 'labels'}    \n",
    "        output_decoded = summary_tokenizer.batch_decode(summary_model.generate(**batch_generate), skip_special_tokens=True)\n",
    "        tc_loss = toxicity_loss_function(output_decoded, device=device)\n",
    "        \n",
    "        # save the toxicity loss\n",
    "        toxic_loss += tc_loss.item()\n",
    "\n",
    "        # the final loss is a linear a combination of the seq2seq loss and the classification loss\n",
    "        final_loss = sq2sq_loss + tc_loss * toxicity_coeff\n",
    "        val_loss += final_loss\n",
    "\n",
    "    # make sure to average the loss across the different batches\n",
    "    summary_loss /= len(val_dataloader)\n",
    "    toxic_loss /= len(val_dataloader)\n",
    "    train_loss /= len(val_dataloader)\n",
    "    \n",
    "    \n",
    "    # make sure to return the losses\n",
    "    return {\"summary_loss\": summary_loss, \"toxic_loss\": toxic_loss, \"val_loss\": train_loss}        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time to write the entire training loop\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from src.training_utilities.exp_tracking import create_summary_writer\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "def _set_summary_writer(writer: SummaryWriter,\n",
    "                        train_losses_dict: Dict, \n",
    "                        val_losses_dict: Dict,\n",
    "                        epoch:int) -> None:\n",
    "    \n",
    "    if writer is not None:\n",
    "        # track the losses in the training stage\n",
    "        writer.add_scalar(main_tag='Train_Losses', \n",
    "                          tag_scalar_dict=train_losses_dict, \n",
    "                          global_step=epoch)\n",
    "\n",
    "        # track the losses in the evaluation stage\n",
    "        writer.add_scalar(main_tag='Validation_Losses', \n",
    "                          tag_scalar_dict=val_losses_dict,\n",
    "                          global_step=epoch)\n",
    "\n",
    "        writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_results(train_losses_dict: Dict, \n",
    "                      val_losses_dict: Dict):\n",
    "    \n",
    "    print(\"Training losses\")\n",
    "    \n",
    "    for k, v in train_losses_dict:\n",
    "        print(f\"{k}: {v}\")\n",
    "\n",
    "    for k, v in val_losses_dict:\n",
    "        print(f\"{k}: {v}\")\n",
    "\n",
    "    print(\"#\" * 50)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.training_utilities.pytorch_utilities import save_model, cleanup\n",
    "\n",
    "def train_custom_summarizer(train_dataloader: DataLoader,\n",
    "                            val_dataloader: DataLoader, \n",
    "                            \n",
    "                            summary_model: AutoModelForSeq2SeqLM, \n",
    "                            summary_tokenizer: AutoTokenizer,\n",
    "                            \n",
    "                            toxicity_loss_function: callable, \n",
    "                            toxicity_coeff: float,\n",
    "                            \n",
    "                            optimizer: torch.optim, \n",
    "                            scheduler: torch.optim.lr_scheduler,\n",
    "\n",
    "                            num_epochs: int = 5,\n",
    "                            device: str = None,\n",
    "                            log_dir: str = None, \n",
    "                            report_per_epoch: int = 5, \n",
    "                            ) -> Tuple[Dict[str, List[float]]]:\n",
    "    \n",
    "    # make sure to cleanup the GPU memory before starting to traing\n",
    "    cleanup()\n",
    "    \n",
    "    # set the default device\n",
    "    device = ('cuda' if torch.cuda.is_available() else 'cpu') if device is None else device \n",
    "    \n",
    "    # set the SummaryWriter for visualization\n",
    "    writer, save_path  = (None, None) if log_dir is None else (create_summary_writer(log_dir, return_path=True))\n",
    "\n",
    "    # save the results somewhere\n",
    "    train_losses = {\"summary_loss\": [], \"toxic_loss\": [], \"train_loss\": []}\n",
    "    val_losses = {\"summary_loss\": [], \"toxic_loss\": [], \"val_loss\": []}\n",
    "\n",
    "    best_train_loss = float('inf')\n",
    "    best_model = None\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        # first the training part\n",
    "        train_results = train_per_epoch(train_dataloader=train_dataloader, \n",
    "                                summary_model=summary_model,\n",
    "                                summary_tokenizer=summary_tokenizer, \n",
    "                                toxicity_loss_function=toxicity_loss_function, \n",
    "                                toxicity_coeff=toxicity_coeff,\n",
    "                                optimizer=optimizer, \n",
    "                                scheduler=scheduler, \n",
    "                                device=device\n",
    "                                )\n",
    "        \n",
    "        val_results = val_per_epoch(val_dataloader=val_dataloader,\n",
    "                                    summary_model=summary_model,\n",
    "                                    summary_tokenizer=summary_tokenizer, \n",
    "                                    toxicity_loss_function=toxicity_loss_function, \n",
    "                                    toxicity_coeff=toxicity_coeff,\n",
    "                                    device=device\n",
    "                                    )\n",
    "\n",
    "        # make sure to save the best model\n",
    "        if best_model is None:\n",
    "            best_model = summary_model\n",
    "\n",
    "        best_model = summary_model if (train_results['train_loss'] < best_train_loss) else best_model\n",
    "        # update 'best_train_loss' \n",
    "        best_train_loss = min([best_train_loss, train_results['train_loss']])\n",
    "\n",
    "        # make sure to track the resutls\n",
    "        for k, v in train_results:\n",
    "            train_losses[k].append(v)\n",
    "\n",
    "        for k, v in val_results:\n",
    "            val_losses[k].append(v)            \n",
    "\n",
    "        # add the results for visualization\n",
    "        _set_summary_writer(writer, \n",
    "                            train_losses_dict=train_results, \n",
    "                            val_losses_dict=val_results)\n",
    "\n",
    "        # report the results if needed  \n",
    "        if epoch % report_per_epoch == 0: \n",
    "            report_results(train_losses_dict=train_results, val_losses_dict=val_results)\n",
    "\n",
    "    # at the end save the model\n",
    "    save_model(model=best_model, path=save_path)\n",
    "    # return the results\n",
    "    return train_losses, val_losses, best_model            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at SkolkovoInstitute/roberta_toxicity_classifier were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from src.evaluation import toxicity_classication as tc\n",
    "singleton_obj = tc.EvalutionSingletonInitializer()\n",
    "tx_classifier, tx_tokenizer, tx_device = singleton_obj.get_toxic_classifier(), singleton_obj.get_toxic_tokenizer(), singleton_obj.get_device()\n",
    "# let's define some of the training parameters\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import LinearLR\n",
    "\n",
    "# lr: the same as the one used in the/home/ayhem18/DEV/My_Kaggle_Repo/pytorch_modular/pytorch_utilities.py\n",
    "optimizer = Adam(MODEL.parameters(), lr=2 * 10 ** -5)\n",
    "scheduler = LinearLR(optimizer=optimizer, start_factor=1, end_factor=0.5,total_iters=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's write a function to compute the summarization + toxicity loss\n",
    "from src.evaluation.toxicity_classication import EvalutionSingletonInitializer\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "def toxic_summary_model_loss(output_decoded: torch.Tensor, \n",
    "                             device):\n",
    "    \n",
    "    singleton_obj = EvalutionSingletonInitializer()\n",
    "    tc_tokenizer, tc_classifier = singleton_obj.get_toxic_tokenizer(), singleton_obj.get_toxic_classifier()\n",
    "\n",
    "    # make sure to freeze their parameters\n",
    "    for p in tx_classifier.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    tc_classifier.to(device)\n",
    "    # tokenize\n",
    "    model_input = tc_tokenizer(output_decoded, return_tensors='pt', padding=True, truncation=True)\n",
    "    # set the input to the device\n",
    "    model_input = {k: v.to(device) for k, v in model_input.items()}\n",
    "    # pass through the model\n",
    "    output = tc_classifier(**model_input)\n",
    "    \n",
    "    loss = torch.mean(softmax(output.logits, dim=1)[:, 1])\n",
    "\n",
    "    loss.requires_grad = True\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Created SummaryWriter, saving to: /home/ayhem18/DEV/TextDetoxification/src/data_analysis/runs/experience_3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ayhem18/DEV/TextDetoxification/env/lib/python3.11/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 7.79 GiB of which 15.75 MiB is free. Including non-PyTorch memory, this process has 7.76 GiB memory in use. Of the allocated memory 7.49 GiB is allocated by PyTorch, and 99.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m/home/ayhem18/DEV/TextDetoxification/src/data_analysis/build_summarizer_train_data.ipynb Cell 33\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ayhem18/DEV/TextDetoxification/src/data_analysis/build_summarizer_train_data.ipynb#X44sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# now the model is ready to train\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/ayhem18/DEV/TextDetoxification/src/data_analysis/build_summarizer_train_data.ipynb#X44sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m _, _, best_model \u001b[39m=\u001b[39m train_custom_summarizer(train_dataloader\u001b[39m=\u001b[39;49mtrain_dl, \n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ayhem18/DEV/TextDetoxification/src/data_analysis/build_summarizer_train_data.ipynb#X44sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m                                             val_dataloader\u001b[39m=\u001b[39;49mval_dl,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ayhem18/DEV/TextDetoxification/src/data_analysis/build_summarizer_train_data.ipynb#X44sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m                                             summary_model\u001b[39m=\u001b[39;49mMODEL,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ayhem18/DEV/TextDetoxification/src/data_analysis/build_summarizer_train_data.ipynb#X44sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m                                             summary_tokenizer\u001b[39m=\u001b[39;49mTOKENIZER, \n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ayhem18/DEV/TextDetoxification/src/data_analysis/build_summarizer_train_data.ipynb#X44sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m                                             toxicity_loss_function\u001b[39m=\u001b[39;49mtoxic_summary_model_loss,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ayhem18/DEV/TextDetoxification/src/data_analysis/build_summarizer_train_data.ipynb#X44sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m                                             toxicity_coeff\u001b[39m=\u001b[39;49m\u001b[39m0.5\u001b[39;49m, \n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ayhem18/DEV/TextDetoxification/src/data_analysis/build_summarizer_train_data.ipynb#X44sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m                                             optimizer\u001b[39m=\u001b[39;49moptimizer, \n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ayhem18/DEV/TextDetoxification/src/data_analysis/build_summarizer_train_data.ipynb#X44sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m                                             scheduler\u001b[39m=\u001b[39;49mscheduler, \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ayhem18/DEV/TextDetoxification/src/data_analysis/build_summarizer_train_data.ipynb#X44sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m                                             num_epochs\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ayhem18/DEV/TextDetoxification/src/data_analysis/build_summarizer_train_data.ipynb#X44sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m                                             report_per_epoch\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ayhem18/DEV/TextDetoxification/src/data_analysis/build_summarizer_train_data.ipynb#X44sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m                                             log_dir\u001b[39m=\u001b[39;49mos\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(HOME, \u001b[39m'\u001b[39;49m\u001b[39mruns\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ayhem18/DEV/TextDetoxification/src/data_analysis/build_summarizer_train_data.ipynb#X44sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m                                             )\n",
      "\u001b[1;32m/home/ayhem18/DEV/TextDetoxification/src/data_analysis/build_summarizer_train_data.ipynb Cell 33\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ayhem18/DEV/TextDetoxification/src/data_analysis/build_summarizer_train_data.ipynb#X44sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, num_epochs \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ayhem18/DEV/TextDetoxification/src/data_analysis/build_summarizer_train_data.ipynb#X44sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m     \u001b[39m# first the training part\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ayhem18/DEV/TextDetoxification/src/data_analysis/build_summarizer_train_data.ipynb#X44sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     train_results \u001b[39m=\u001b[39m train_per_epoch(train_dataloader\u001b[39m=\u001b[39mtrain_dataloader, \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ayhem18/DEV/TextDetoxification/src/data_analysis/build_summarizer_train_data.ipynb#X44sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m                             summary_model\u001b[39m=\u001b[39msummary_model,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ayhem18/DEV/TextDetoxification/src/data_analysis/build_summarizer_train_data.ipynb#X44sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m                             summary_tokenizer\u001b[39m=\u001b[39msummary_tokenizer, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ayhem18/DEV/TextDetoxification/src/data_analysis/build_summarizer_train_data.ipynb#X44sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m                             device\u001b[39m=\u001b[39mdevice\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ayhem18/DEV/TextDetoxification/src/data_analysis/build_summarizer_train_data.ipynb#X44sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m                             )\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/ayhem18/DEV/TextDetoxification/src/data_analysis/build_summarizer_train_data.ipynb#X44sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m     val_results \u001b[39m=\u001b[39m val_per_epoch(val_dataloader\u001b[39m=\u001b[39;49mval_dataloader,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ayhem18/DEV/TextDetoxification/src/data_analysis/build_summarizer_train_data.ipynb#X44sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m                                 summary_model\u001b[39m=\u001b[39;49msummary_model,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ayhem18/DEV/TextDetoxification/src/data_analysis/build_summarizer_train_data.ipynb#X44sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m                                 summary_tokenizer\u001b[39m=\u001b[39;49msummary_tokenizer, \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ayhem18/DEV/TextDetoxification/src/data_analysis/build_summarizer_train_data.ipynb#X44sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m                                 toxicity_loss_function\u001b[39m=\u001b[39;49mtoxicity_loss_function, \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ayhem18/DEV/TextDetoxification/src/data_analysis/build_summarizer_train_data.ipynb#X44sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m                                 toxicity_coeff\u001b[39m=\u001b[39;49mtoxicity_coeff,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ayhem18/DEV/TextDetoxification/src/data_analysis/build_summarizer_train_data.ipynb#X44sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m                                 device\u001b[39m=\u001b[39;49mdevice\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ayhem18/DEV/TextDetoxification/src/data_analysis/build_summarizer_train_data.ipynb#X44sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m                                 )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ayhem18/DEV/TextDetoxification/src/data_analysis/build_summarizer_train_data.ipynb#X44sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m     \u001b[39m# make sure to save the best model\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ayhem18/DEV/TextDetoxification/src/data_analysis/build_summarizer_train_data.ipynb#X44sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m     \u001b[39mif\u001b[39;00m best_model \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;32m/home/ayhem18/DEV/TextDetoxification/src/data_analysis/build_summarizer_train_data.ipynb Cell 33\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ayhem18/DEV/TextDetoxification/src/data_analysis/build_summarizer_train_data.ipynb#X44sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m batch \u001b[39m=\u001b[39m {k: v\u001b[39m.\u001b[39mto(device) \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m batch\u001b[39m.\u001b[39mitems()}\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ayhem18/DEV/TextDetoxification/src/data_analysis/build_summarizer_train_data.ipynb#X44sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# the next step is to pass the data to the model \u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/ayhem18/DEV/TextDetoxification/src/data_analysis/build_summarizer_train_data.ipynb#X44sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m model_output \u001b[39m=\u001b[39m summary_model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mbatch)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ayhem18/DEV/TextDetoxification/src/data_analysis/build_summarizer_train_data.ipynb#X44sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m# extract the sequence to sequence loss\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ayhem18/DEV/TextDetoxification/src/data_analysis/build_summarizer_train_data.ipynb#X44sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m sq2sq_loss \u001b[39m=\u001b[39m model_output\u001b[39m.\u001b[39mloss \n",
      "File \u001b[0;32m~/DEV/TextDetoxification/env/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/DEV/TextDetoxification/env/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/DEV/TextDetoxification/env/lib/python3.11/site-packages/transformers/models/t5/modeling_t5.py:1781\u001b[0m, in \u001b[0;36mT5ForConditionalGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1779\u001b[0m     \u001b[39m# move labels to correct device to enable PP\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m     labels \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39mto(lm_logits\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m-> 1781\u001b[0m     loss \u001b[39m=\u001b[39m loss_fct(lm_logits\u001b[39m.\u001b[39;49mview(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, lm_logits\u001b[39m.\u001b[39;49msize(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)), labels\u001b[39m.\u001b[39;49mview(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m))\n\u001b[1;32m   1782\u001b[0m     \u001b[39m# TODO(thom): Add z_loss https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/layers.py#L666\u001b[39;00m\n\u001b[1;32m   1784\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m return_dict:\n",
      "File \u001b[0;32m~/DEV/TextDetoxification/env/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/DEV/TextDetoxification/env/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/DEV/TextDetoxification/env/lib/python3.11/site-packages/torch/nn/modules/loss.py:1179\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1178\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m-> 1179\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mcross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m   1180\u001b[0m                            ignore_index\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore_index, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction,\n\u001b[1;32m   1181\u001b[0m                            label_smoothing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabel_smoothing)\n",
      "File \u001b[0;32m~/DEV/TextDetoxification/env/lib/python3.11/site-packages/torch/nn/functional.py:3053\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3051\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3052\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3053\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcross_entropy_loss(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 7.79 GiB of which 15.75 MiB is free. Including non-PyTorch memory, this process has 7.76 GiB memory in use. Of the allocated memory 7.49 GiB is allocated by PyTorch, and 99.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# now the model is ready to train\n",
    "_, _, best_model = train_custom_summarizer(train_dataloader=train_dl, \n",
    "                                            val_dataloader=val_dl,\n",
    "                                            summary_model=MODEL,\n",
    "                                            summary_tokenizer=TOKENIZER, \n",
    "                                            toxicity_loss_function=toxic_summary_model_loss,\n",
    "                                            toxicity_coeff=0.5, \n",
    "                                            optimizer=optimizer, \n",
    "                                            scheduler=scheduler, \n",
    "                                            num_epochs=3,\n",
    "                                            report_per_epoch=1,\n",
    "                                            log_dir=os.path.join(HOME, 'runs')\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
