{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import os\n",
    "from pathlib import Path\n",
    "HOME = os.getcwd()\n",
    "DATA_FOLDER = os.path.join(Path(HOME).parent, 'data')\n",
    "data_path = os.path.join(DATA_FOLDER, 'fixed.csv')\n",
    "\n",
    "# step1: load the dataset \n",
    "dataset = load_dataset(\"csv\", data_files=data_path)\n",
    "dataset = dataset.remove_columns(['similarity', 'lenght_diff', 'source_tox', 'target_tox'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['source', 'target'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sample = dataset['train'].shuffle(seed=69).select(range(1000))\n",
    "data_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step2: create the model and the tokenizer\n",
    "import torch\n",
    "from transformers import T5TokenizerFast, AutoModelForSeq2SeqLM\n",
    "CHECKPOINT = 't5-small'\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "tokenizer = T5TokenizerFast.from_pretrained(CHECKPOINT)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(CHECKPOINT).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step3 create a function to tokenize the data\n",
    "TASK_PREFIX = 'summarize: '\n",
    "\n",
    "def prepare_data(batch, split: str ='source'):\n",
    "    tok_batch = [TASK_PREFIX + s for s in batch[split]]\n",
    "    return tokenizer(tok_batch, truncation=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_tokenized = data_sample.map(prepare_data, batched=True)\n",
    "target_tokenized = data_sample.map(lambda x: prepare_data(x, split='target'), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['source', 'target', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 1000\n",
      "})\n",
      "Dataset({\n",
      "    features: ['source', 'target', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 1000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(source_tokenized)\n",
    "# 'soure' and 'target' columns are unnecessary for the 'source_tokenized' dataset\n",
    "print(target_tokenized)\n",
    "# 'source', 'target, and 'attention_masks' are unncessary for the 'target_tokenized'\n",
    "source_tokenized = source_tokenized.remove_columns(['source', 'target'])\n",
    "target_tokenized = target_tokenized.remove_columns(['source', 'target', 'attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create 2 dataloaders, one for source (will be used for predictions) and one for target\n",
    "import torch\n",
    "from transformers import DataCollatorWithPadding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "# create a dataloader to pass the data to the model\n",
    "source_dl = DataLoader(dataset=source_tokenized, batch_size=64, shuffle=False, collate_fn=data_collator)\n",
    "target_dl = DataLoader(dataset=target_tokenized, batch_size=64, shuffle=False, collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[21603,    10,    27,  ...,     0,     0,     0],\n",
       "        [21603,    10,  2087,  ...,     0,     0,     0],\n",
       "        [21603,    10,   216,  ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [21603,    10,  1563,  ...,     0,     0,     0],\n",
       "        [21603,    10, 10855,  ...,     0,     0,     0],\n",
       "        [21603,    10,   264,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(source_dl))\n",
    "next(iter(target_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading pytorch_model.bin: 100%|██████████| 501M/501M [00:38<00:00, 12.9MB/s] \n",
      "Some weights of the model checkpoint at SkolkovoInstitute/roberta_toxicity_classifier were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: 'neutral', 1: 'toxic'}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, \\\n",
    "    RobertaTokenizer, RobertaForSequenceClassification\n",
    "\n",
    "model = RobertaForSequenceClassification.from_pretrained('SkolkovoInstitute/roberta_toxicity_classifier')\n",
    "model.config.id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 0 examples [00:00, ? examples/s]/home/ayhem18/DEV/TextDetoxification/env/lib/python3.11/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Generating train split: 1000 examples [00:06, 148.09 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from \n",
    "import re\n",
    "\n",
    "def build_dataset():\n",
    "    for source_b, target_b in zip(source_dl, target_dl):\n",
    "        # ignore the source data\n",
    "        model_batch = {k: v.to(DEVICE) for k, v in source_b.items()}\n",
    "        # pass the batch to the model\n",
    "        output = model.generate(**model_batch)\n",
    "        # print(output)\n",
    "        output_decoded = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
    "        source = tokenizer.batch_decode(source_b['input_ids'], skip_special_tokens=True)\n",
    "        target = tokenizer.batch_decode(target_b['input_ids'], skip_special_tokens=True)\n",
    "\n",
    "        # the summary's toxicity classification is next:\n",
    "        summary_tox = \n",
    "\n",
    "        for text, source_text, target_text in zip(output_decoded, source, target):\n",
    "            yield {\"source\": re.sub(TASK_PREFIX, \"\", source_text), \"target\": re.sub(TASK_PREFIX, \"\", target_text), \"summary\": text}\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "summarized_dataset = Dataset.from_generator(build_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'I was away for too damn long, sir.',\n",
       " 'target': \"I've been away too long, sir.\",\n",
       " 'summary': 'i was away for too damn long, sir.'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarized_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 196.58ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "152714"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the dataset \n",
    "summarized_dataset.to_csv(os.path.join(DATA_FOLDER, 'summarized.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(os.path.join(DATA_FOLDER, 'summarized.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I was away for too damn long, sir.</td>\n",
       "      <td>I've been away too long, sir.</td>\n",
       "      <td>i was away for too damn long, sir.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Maybe they'il find your Ripper costume buried ...</td>\n",
       "      <td>maybe they'll find the Ripper costume buried n...</td>\n",
       "      <td>buried with your condor outfit.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>shit, he's right there!</td>\n",
       "      <td>He's right there!</td>\n",
       "      <td>shit, he's right there!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>moron.</td>\n",
       "      <td>Prick.</td>\n",
       "      <td>moron.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>If you can't get what you want with foreign ai...</td>\n",
       "      <td>if you don't get what you want, through foreig...</td>\n",
       "      <td>if you can't get what you want with foreign ai...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              source  \\\n",
       "0                 I was away for too damn long, sir.   \n",
       "1  Maybe they'il find your Ripper costume buried ...   \n",
       "2                            shit, he's right there!   \n",
       "3                                             moron.   \n",
       "4  If you can't get what you want with foreign ai...   \n",
       "\n",
       "                                              target  \\\n",
       "0                      I've been away too long, sir.   \n",
       "1  maybe they'll find the Ripper costume buried n...   \n",
       "2                                  He's right there!   \n",
       "3                                             Prick.   \n",
       "4  if you don't get what you want, through foreig...   \n",
       "\n",
       "                                             summary  \n",
       "0                 i was away for too damn long, sir.  \n",
       "1                    buried with your condor outfit.  \n",
       "2                            shit, he's right there!  \n",
       "3                                             moron.  \n",
       "4  if you can't get what you want with foreign ai...  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
