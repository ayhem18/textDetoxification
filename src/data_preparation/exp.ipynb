{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from pathlib import Path\n",
    "HOME = os.getcwd()\n",
    "DATA_FOLDER = os.path.join(Path(HOME).parent, 'data')\n",
    "data_path = os.path.join(DATA_FOLDER, 'filtered.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "current = HOME \n",
    "while 'src' not in os.listdir(current):\n",
    "    current = Path(current).parent\n",
    "\n",
    "sys.path.append(str(current))\n",
    "sys.path.append(os.path.join(str(current), 'data_analysis'))\n",
    "sys.path.append(os.path.join(str(current), 'evaluation'))\n",
    "sys.path.append(os.path.join(str(current), 'text_processing'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ayhem18/DEV/TextDetoxification/env/lib/python3.11/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# let's first fix the data \n",
    "import src.data_preparation.prepare_data as prd \n",
    "fixed_data = prd.fix_initial_data(data_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['it', 'might', 'appear', 'counter', '-', 'intuitive', ',', 'but', 'amazingly', 'it', 'is', 'true', '.']\n",
      "[2009, 2453, 3711, 4675, 1011, 29202, 1010, 2021, 29350, 2009, 2003, 2995, 1012]\n",
      "True\n",
      "['it', 'might', 'appear', 'counter', '-', 'intuitive', ',', 'but', 'amazingly', 'it', 'is', 'true', '.']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, DistilBertModel\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['it', 'is', 'relative']\n",
      "[2009, 2003, 5816]\n",
      "True\n",
      "['it', 'is', 'relative']\n",
      "['it', 'is', 'relatively', 'strange']\n",
      "[2009, 2003, 4659, 4326]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "int() argument must be a string, a bytes-like object or a real number, not 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/ayhem18/DEV/TextDetoxification/src/data_preparation/exp.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ayhem18/DEV/TextDetoxification/src/data_preparation/exp.ipynb#W4sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m ids \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mconvert_tokens_to_ids(ts)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ayhem18/DEV/TextDetoxification/src/data_preparation/exp.ipynb#W4sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mprint\u001b[39m(ids)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/ayhem18/DEV/TextDetoxification/src/data_preparation/exp.ipynb#W4sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mprint\u001b[39m(tokenizer\u001b[39m.\u001b[39;49mconvert_ids_to_tokens([ids]))\n",
      "File \u001b[0;32m~/DEV/TextDetoxification/env/lib/python3.11/site-packages/transformers/tokenization_utils_fast.py:389\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast.convert_ids_to_tokens\u001b[0;34m(self, ids, skip_special_tokens)\u001b[0m\n\u001b[1;32m    387\u001b[0m tokens \u001b[39m=\u001b[39m []\n\u001b[1;32m    388\u001b[0m \u001b[39mfor\u001b[39;00m index \u001b[39min\u001b[39;00m ids:\n\u001b[0;32m--> 389\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mint\u001b[39;49m(index)\n\u001b[1;32m    390\u001b[0m     \u001b[39mif\u001b[39;00m skip_special_tokens \u001b[39mand\u001b[39;00m index \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mall_special_ids:\n\u001b[1;32m    391\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a real number, not 'list'"
     ]
    }
   ],
   "source": [
    "sentence = [\"It is relative\"]\n",
    "ts = tokenizer.tokenize(sentence[0])\n",
    "print(ts) \n",
    "ids = tokenizer.convert_tokens_to_ids(ts)\n",
    "print(ids)\n",
    "print(len(ts) == len(ids))\n",
    "print(tokenizer.convert_ids_to_tokens(ids))\n",
    "\n",
    "s = \"It is relatively strange\"\n",
    "ts = tokenizer.tokenize(s)\n",
    "print(ts) \n",
    "ids = tokenizer.convert_tokens_to_ids(ts)\n",
    "print(ids)\n",
    "print(tokenizer.convert_ids_to_tokens(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
